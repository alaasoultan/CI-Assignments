# first, hill climbing algorithm implementation:

def hill_climbing_tsp(num_cities, max_iterations):
    # Initialize a random path
    current_path = list(range(num_cities))
    random.shuffle(current_path)
    
    # Calculate the initial distance
    current_distance = calculate_distance(current_path)
    
    for _ in range(max_iterations):
        # Generate a neighboring path by swapping two cities
        neighbor_path = current_path.copy()
        i, j = random.sample(range(num_cities), 2)
        neighbor_path[i], neighbor_path[j] = neighbor_path[j], neighbor_path[i]
        
        # Calculate the distance of the neighbor path
        neighbor_distance = calculate_distance(neighbor_path)
        
        # If the neighbor path is better, move to it
        if neighbor_distance < current_distance:
            current_path = neighbor_path
            current_distance = neighbor_distance
    
    # Print the optimal path and total distance
    print("Optimal path:", current_path)
    print("Total distance:", current_distance)

def calculate_distance(path):
    # Calculate the total distance of the path 
    total_distance = 0
    for i in range(len(path) - 1):
        total_distance += euclidean_distance(cities[path[i]], cities[path[i + 1]])
    return total_distance

def euclidean_distance(city1, city2):
    # Calculate Euclidean distance between two cities
    return math.sqrt((city1[0] - city2[0])**2 + (city1[1] - city2[1])**2)

if __name__ == "__main__":
    # Example usage: TSP with 10 cities
    NUM_CITIES = 10
    cities = [(random.uniform(0, 100), random.uniform(0, 100)) for _ in range(NUM_CITIES)]
    max_iterations = 10000
    hill_climbing_tsp(NUM_CITIES, max_iterations)


#second, gradient decent algorithm implementation:

import numpy as np

def gradient_descent(x, y, alpha, num_iters):

    m = len(y)  # Number of training examples
    n = x.shape[1]  # Number of features (including bias term)
    weights = np.zeros(n)  # Initialize weights to zeros
    
    for _ in range(num_iters):
        # Calculate predictions
        predictions = np.dot(x, weights)
        
        # Compute the gradient (partial derivatives) of the cost function
        gradient = (1 / m) * np.dot(x.T, (predictions - y))
        
        # Update weights
        weights -= alpha * gradient
    
    return weights

# Example usage:
if __name__ == "__main__":
    # Generate some synthetic data for demonstration
    num_samples = 100
    x = np.random.rand(num_samples, 2)  # Two features
    true_weights = np.array([2, 3])  # True weights
    y = np.dot(x, true_weights) + np.random.randn(num_samples) * 0.5  # Add noise
    
    # Add bias term (intercept)
    x_bias = np.c_[np.ones(num_samples), x]
    
    # Set hyperparameters
    learning_rate = 0.01
    iterations = 1000
    
    # Run gradient descent
    optimized_weights = gradient_descent(x_bias, y, learning_rate, iterations)
    
    print("Optimized weights:", optimized_weights)

# third, simulated annealing algorithm implementation:

import math
import random

def objective_function(x):
   
    # Sphere function
    return sum([(i**2) for i in x])

def random_solution(bounds):
  
    return [random.uniform(bounds[i][0], bounds[i][1]) for i in range(len(bounds))]

def next_solution(current_solution, step_size, bounds):
    
    return [max(min(current_solution[i] + random.uniform(-step_size, step_size), bounds[i][1]), bounds[i][0]) for i in range(len(bounds))]

def simulated_annealing(Tmax, Tmin, Cinit, step_size, max_iterations):
  
    T = Tmax
    current_solution = Cinit
    current_energy = objective_function(current_solution)

    while T > Tmin:
        for _ in range(max_iterations):
            next_solution_candidate = next_solution(current_solution, step_size, bounds)
            next_energy = objective_function(next_solution_candidate)
            delta_energy = next_energy - current_energy

            if delta_energy > 0:
                current_solution = next_solution_candidate
                current_energy = next_energy
            else:
                if math.exp(delta_energy / T) > random.uniform(0, 1):
                    current_solution = next_solution_candidate
                    current_energy = next_energy

        T *= 0.95  # Cooling schedule 

    return current_solution, current_energy

# Example usage
bounds = [(-5, 5), (-5, 5)]  # Bounds for each dimension
Tmax = 1000
Tmin = 1e-3
Cinit = random_solution(bounds)
step_size = 0.1
max_iterations = 1000

best_solution, best_fitness = simulated_annealing(Tmax, Tmin, Cinit, step_size, max_iterations)

print("Best solution:", best_solution)
print("Best fitness:", best_fitness)
